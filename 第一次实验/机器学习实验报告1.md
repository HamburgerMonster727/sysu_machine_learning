# 机器学习实验报告1

## 1.实现逻辑回归线性分类器

先读入样本特征和样本类别

```matlab
%%读入数据，开始训练逻辑回归线性分类器
train_data = load('train_banknote.txt');
% X为样本特征，具有两个特征，Y为样本类别
X = train_data(:, [1,2]); Y = train_data(:, 5);
```

因为回归方程中具有常数项，所以需要往X样本特征添加一个常数项1

```matlab
% 记录样本矩阵X的大小
[m, n] = size(X);
% 样本特征中，添加一个常数特征，全部为1
X = [ones(m, 1) X];
```

根据损失方程L(w) = -ylog(σ(xw))-(1-y)log(1-σ(xw))，建立损失函数，计算得到梯度和lost值

```matlab
%% 自定义损失函数，得到lost值和梯度值
function [lost, gradient] = costFunction(theta, X, Y)
    n = length(Y); % 训练样本的数目
    lost = 0;
    gradient = zeros(size(theta));
    sig = sigmoid(X * theta);
    lost = 1/n * (-Y' * log(sig) - (1 - Y') * log(1 - sig)); % 计算lost
    gradient = 1/n * (X' * (sig - Y)); % 计算梯度
end

%% sigmoid函数
function ans = sigmoid(X)
    ans = zeros(size(X));
    ans = 1 ./ (1 + exp(-X));
end
```

使用fminunc函数，不断迭代计算，梯度下降，得到最小的lost值和theta矩阵

```matlab
% 创建初始化theta矩阵，大小为(n+1)*1
initial_theta = zeros(n + 1, 1);
% 使用自定义损失函数，计算得到梯度和lost
[lost, gradient] = costFunction(initial_theta, X, Y);
% 通过设置options，把fminunc的迭代次数设置为400
options = optimset('GradObj', 'off', 'MaxIter', 400);
% 使用fminunc函数，不断梯度下降，得到lost最小时的thete矩阵
[theta, lost] = fminunc(@(t)(costFunction(t, X, Y)), initial_theta, options);
```

根据theta矩阵，画出决策边界，根据样本类别，展示样本数据

```matlab
%% 按类别展示样本数据
function showData(X, Y)    
    figure;
    hold on;
    class0 = find(Y == 0);
    class1 = find(Y == 1);
    plot(X(class0,1), X(class0,2), 'ro');
    plot(X(class1,1), X(class1,2), 'go');
    axis([-8, 8, -15, 15])
    xlabel('LR linear classifier')
    hold off;
end

%% 展示决策边界
function showDecisionBoundary(theta, X, Y)
    % 展示类别数据
    showData(X(:,2:3), Y);
    hold on
    % 计算得到决策边界的两个点，画出一条线
    plot_x = [min(X(:,2)) - 2,  max(X(:,2)) + 2];
    plot_y = (-1 ./ theta(3)) .* (theta(2) .* plot_x + theta(1));
    plot(plot_x, plot_y)
    hold off
end
```

根据theta矩阵和训练样本，计算训练样本的分类精度

```matlab
% 计算得到训练样本的分类精度
train_acc = accuracy(theta, X);
fprintf('train accuracy: %f\n', mean(double(train_acc == Y)) * 100);

%% 根据训练得到的theta矩阵，把样本计算一次，得到结果，用于比较
function acc = accuracy(theta, X)
    m = size(X, 1); % Number of training examples
    acc = zeros(m, 1);
    acc = round(sigmoid(X * theta));
end
```

根据theta矩阵和测试样本，计算测试样本的分类精度

```matlab
% 计算得到测试样本的分类精度
test_data = load("test_banknote.txt");
test_X = test_data(:, [1,2]); test_Y = test_data(:, 5);
[m, n] = size(test_X);
test_X = [ones(m, 1) test_X];
test_acc = accuracy(theta, test_X);
fprintf('test accuracy: %f\n', mean(double(test_acc == test_Y)) * 100);
```

## 2.使用基函数修改1得到非线性分类器

读入样本特征后，使用基函数，对样本特征进行修改

```matlab
%使用基函数
for i = 1 : m
    tmp = X(i,1);
    X(i,1) = X(i,1) + X(i,2);
    X(i,2) = tmp * X(i,2);
end
```

然后使用1题的线性分类器，进行计算得到theta矩阵

## 3.通过L1和L2范数对2进行正则化

要实现L1,L2正则化，需要对损失函数进行修改，lambda系数设置为1

根据L1正则化方程，修改得到：

```matlab
%% 自定义损失函数，得到lost值和梯度值
function [lost, gradient] = costFunction(theta, X, Y)
    lambda = 1;
    L1 = 0;
    for i = 1 : size(theta,1)
        L1 = L1 + abs(theta(i,1));
    end
    L1 = lambda * L1;
    
    n = length(Y); % 训练样本的数目
    lost = 0;
    gradient = zeros(size(theta));
    sig = sigmoid(X * theta);
    lost = (1/n * (-Y' * log(sig) - (1 - Y') * log(1 - sig))) + L1; % 计算lost
    gradient = 1/n * (X' * (sig - Y)); % 计算梯度
end
```

根据L2正则化方程，修改得到：

```matlab
%% 自定义损失函数，得到lost值和梯度值
function [lost, gradient] = costFunction(theta, X, Y)
    lambda = 1;
    L2 = 0;
    for i = 1 : size(theta,1)
        L2 = L2 + (theta(i,1) * theta(i,1));
    end
    L2 = lambda * sqrt(L2);
    
    n = length(Y); % 训练样本的数目
    lost = 0;
    gradient = zeros(size(theta));
    sig = sigmoid(X * theta);
    lost = (1/n * (-Y' * log(sig) - (1 - Y') * log(1 - sig))) + L2; % 计算lost
    gradient = 1/n * (X' * (sig - Y)); % 计算梯度
end
```

## 4.对实验结果进行分析

### 训练数据和测试数据

选择的实验数据来自https://archive.ics.uci.edu/ml/datasets/banknote+authentication，拥有两类数据，五种样本特征，我们只选择其中两种样本特征。 把下载的数据分为两部分，一部分作为训练数据，用于训练分类器，另一部分作为测试数据，用于测试分类器的分类精度。

### 线性分类器

![image-20220327230359525](C:\Users\菜徐鲲\AppData\Roaming\Typora\typora-user-images\image-20220327230359525.png)

![image-20220328135708323](C:\Users\菜徐鲲\AppData\Roaming\Typora\typora-user-images\image-20220328135708323.png)

### 基函数非线性分类器

![image-20220327230443557](C:\Users\菜徐鲲\AppData\Roaming\Typora\typora-user-images\image-20220327230443557.png)

![image-20220328135718987](C:\Users\菜徐鲲\AppData\Roaming\Typora\typora-user-images\image-20220328135718987.png)

### L1正则化：

![image-20220327230523231](C:\Users\菜徐鲲\AppData\Roaming\Typora\typora-user-images\image-20220327230523231.png)

![image-20220328135728664](C:\Users\菜徐鲲\AppData\Roaming\Typora\typora-user-images\image-20220328135728664.png)

### L2正则化：

![image-20220327230539792](C:\Users\菜徐鲲\AppData\Roaming\Typora\typora-user-images\image-20220327230539792.png)

![image-20220328135736890](C:\Users\菜徐鲲\AppData\Roaming\Typora\typora-user-images\image-20220328135736890.png)

### 实验分析

本次实验的实验样本，从图片可以看到，两类的数据具有部分重叠的区域。对此实验样本，线性分类器的分类效果是最好的，训练分类精度有89.2%，测试分类精度也有87.6%，测试的分类精度并没有达到训练分类精度的精准度。

在非线性分类器中，经过我多次的选择，最终选择的基函数为【x1+x2, x1*x2】，得到的训练分类精度为77.9%，测试分类精度为77.5%，测试的分类精度并没有达到训练分类精度的精准度，但两者已经十分接近。

要实现L1正则化，需要对损失函数进行修改，需要加上theta矩阵的每个元素的绝对值，得到的训练分类精度为74.7%，测试分类精度为75.2%，测试精度比训练精度高，但却没有未进行L1正则化的测试精度高，不知为何。

要实现L2正则化，需要对损失函数进行修改，需要加上theta矩阵的每个元素的平方，得到的和再开方，得到的训练分类精度为76.4%，测试分类精度为79.7%，测试精度比训练精度高，也比未进行L2正则化的测试精度高。

从本次实验的结果可以看到，非线性分类器的分类精度受所选择的基函数的影响很大，也有训练样本的因素所在，如何选择良好的基函数，使得非线性分类器的分类精度更加精确是一大难题。L1和L2正则化对非线性分类器的分类精度都有所影响，本次实验中选择L2正则化效果更加，不同的实验样本有不同的效果。